---
title: "Analysis of Seasonality"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Setup
```{r setup, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Setting home directory for development. Since we are using knitr, we have to do things differently.
require("knitr")
nodename = Sys.info()['nodename']
if (grepl('SKYLLA', nodename)){
  Sys.setlocale("LC_TIME", "C")
  opts_knit$set(root.dir = "G:/Dev/DataScience/TSA-Finance/data")
} else if (grepl('ARES', nodename)) {
  Sys.setlocale("LC_TIME", "C")
  opts_knit$set(root.dir = "C:/Users/Pascal/Documents/Repository/DataScience/TSA-Finance/data")
} else {
  opts_knit$set(root.dir = "~/Code/TSA-Finance/data")
}
```
```{r, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(xts)
library(TSA)
library(TTR)
library(dplyr)
library(fBasics)
library(forecast)
library(ggfortify)
library(collections)
library(matrixStats)
```

Common functions
```{r}
# Returns the bigger date vector of to given vectors. Requires date vector in format %Y %j.
get_bigger_date_vector <- function(date1, date2) {
  if(date1[1] > date2[1]){
    return(date1)
  }else if(date1[1] < date2[1]){
    return(date2)
  }else{
    if(date1[2] > date2[2]){
      return(date1)
    }else if(date1[2] < date2[2]){
      return(date2)
    }else{
      return(date1)
    }
  }
}
# Returns the smaller date vector of to given vectors. Requires date vector in format %Y %j.
get_smaller_date_vector <- function(date1, date2){
  result <- get_bigger_date_vector(date1, date2)
  if (result == date1){
    return(date2)
  }else{
    return(date1)
  }
}
# Allows for each row functionality on dataframes.
rows = function(tab) lapply(
  seq_len(nrow(tab)),
  function(i) unclass(tab[i,,drop=F])
)
```

# Preparation
## Loading
The top 50 currencies by market cap are stored as timeseries in a dictionary like structure in the file.
```{r}
tso.top50 <- readRDS(file = "tso.top50.rds")
```

## Trimming
Not all timeseries have exactly the same start and end date. Finding the last possible start date and the earliest possible end date is done in the next chunk.
```{r message=FALSE, warning=FALSE}
max_start_date <- c(1970, 1)
min_end_date <- c(2999, 300)
for (currency in tso.top50$keys()) {
  currency.ts <- tso.top50$get(currency)
  max_start_date <- get_bigger_date_vector(start(currency.ts), max_start_date)
  min_end_date <- get_smaller_date_vector(end(currency.ts), min_end_date)
}
```

After we have found said dates, we can then now trim the timeseries to those dates. All timeseries should have the same dimensions after this.
```{r}
currency.tsos <- Dict()
length.tso <- 0

for (currency in tso.top50$keys()) {
  currency.ts <- tso.top50$get(currency)
  sub.ts <- window(currency.ts, start=max_start_date, end=min_end_date)
  length.tso <- length(sub.ts)
  currency.tsos$set(currency, sub.ts)
}
```

# Analysis
## Decomposition

We decompose all currencies and store the seasonality data in a dataframe.
```{r}
currency.df <- data.frame(matrix(ncol = length(currency.tsos$keys()), nrow = length.tso))
colnames(currency.df) <- currency.tsos$keys()

# Decompose all currencies
for (currency in currency.tsos$keys()){
  decomposed <- decompose(currency.tsos$get(currency))
  currency.df[currency] <- as.numeric(decomposed$seasonal)
}

```

## Windowing
The seasonality of a timeseries is always 0 centered. Thus, it would allow us to compare all seasonal components of the currencies. The idea is, that if there are days which influenced the whole crypto market, then we would see those in the seasonal components by them acting similar.
```{r}
#Â Calculate some stats row wise.
currency.df <- transform(currency.df, row.sd=apply(currency.df, 1, sd, na.rm=TRUE))
currency.df <- transform(currency.df, row.mean=apply(currency.df, 1, mean, na.rm=TRUE))
```

In our analysis, we consider days as impactful or common amongst the currencies, if the standard deviation accross currencies of a given day, does not exceed a certain limit (window). 
```{r}
window.size <- .2 # factor of minimal standard deviation
min.sd <- min(currency.df$row.sd)
currency.df <- mutate(currency.df, in.window = seq(from=FALSE, by=FALSE,length.out =  length.tso))

currency.df$in.window <- apply(currency.df,1, function(row) { row[['row.sd']] <= min.sd * (1 + window.size)} )

```

We can have a look at the days which are in the window or outside of the window. For the sake of simplicity we are looking at the top 5 currencies.
```{r}
mfrow=c(5,1)
for(ccy in c('BTC', 'ETH', 'XRP', 'BCH', 'LTC')) {
  row.names(currency.df) <- seq(from = as.Date(toString(max_start_date), '%Y, %j'), by = "day", length.out = length.tso)
  rn <- row.names(currency.df)
  plot.data.out.window <- data.frame(matrix(ncol = 1, nrow = length.tso))
  plot.data.in.window <- data.frame(matrix(ncol = 1, nrow = length.tso))
  row.names(plot.data.out.window) <- rn
  row.names(plot.data.in.window) <- rn
  colnames(plot.data.out.window) <- c(ccy)
  colnames(plot.data.in.window) <- c(ccy)
  
  currency.df.bkp <- currency.df
  currency.df[currency.df$in.window == TRUE,][[ccy]] <- NA
  plot.data.out.window[[ccy]] <- currency.df[[ccy]]
  currency.df <- currency.df.bkp
  currency.df[currency.df$in.window == FALSE,][[ccy]] <- NA
  plot.data.in.window[[ccy]] <- currency.df[[ccy]]
  currency.df <- currency.df.bkp
  
  par(mar=c(7,4,4,2))
  day.interval <- 90
  plot(plot.data.out.window[[ccy]],
       type='l',
       main=ccy,
       col='darkgoldenrod1',
       lwd=1,
       ylab = 'Seasonality',
       xaxt = 'n', xlab=''
       )
  axis(1, at=seq(from = 0, to = length.tso, by = day.interval), labels=rn[seq(1,length(rn), day.interval)], las=2)
  abline(v=seq(from = 0, to = length.tso, by = day.interval))
  lines(plot.data.in.window[[ccy]], col='purple', lwd=3)
  legend('bottomright', legend=c('Common', 'Unique'), col=c('purple', 'darkgoldenrod1'), lty=1)
}
```

## Direction of slope
Analysis of derivative of consecutive days. TBD

## Common dates
Thanks to the windowing method, we have identified days that are "common" amongst currencies. Let's quantify these days and see how they spread over a year.
```{r}
row.names(currency.df) <- seq(from = as.Date(toString(max_start_date), '%Y, %j'), by = "day", length.out = length.tso)
common.dates <- row.names(currency.df[currency.df$in.window == TRUE,])
common.dates <- as.Date(common.dates, format="%Y-%m-%d")

hist(as.integer(format(common.dates, format = '%j')), 
     breaks = 73,
     main='Frequency of common days in an average year',
     xlim=c(1,365),
     ylim=c(0,20),
     xlab='Day of the Year', xaxt='n',
     col='gray')
axis(side=1, at=seq(0,365, 5), labels=seq(0,365,5), las=2)
abline(v=c(0,30.4167,60.8334,91.2501,121.6668,152.0835,182.5002,212.9169,243.3336,273.7503,304.167,334.5837))
text(0, 15, 'January', srt=90, col = "gray60", adj = c(0, -.15))
text(30.4167, 15, 'February', srt=90, col = "gray60", adj = c(0, -.15))
text(60.8334, 15, 'March', srt=90, col = "gray60", adj = c(0, -.15))
text(91.2501, 15, 'April', srt=90, col = "gray60", adj = c(0, -.15))
text(121.6668, 15, 'May', srt=90, col = "gray60", adj = c(0, -.15))
text(152.0835, 15, 'June', srt=90, col = "gray60", adj = c(0, -.15))
text(182.5002, 15, 'July', srt=90, col = "gray60", adj = c(0, -.15))
text(212.9169, 15, 'August', srt=90, col = "gray60", adj = c(0, -.15))
text(243.3336, 15, 'September', srt=90, col = "gray60", adj = c(0, -.15))
text(273.7503, 15, 'October', srt=90, col = "gray60", adj = c(0, -.15))
text(304.167, 15, 'November', srt=90, col = "gray60", adj = c(0, -.15))
text(334.5837, 15, 'December', srt=90, col = "gray60", adj = c(0, -.15))
```

# Conclusion
We can see a big accumulation of common days within march and april. Some research suggests, that these days are in line with the fact that people have to pay their tax statements in most countries.